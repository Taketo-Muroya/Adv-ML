{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3 for Adv ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taketo-Muroya/Adv-ML/blob/master/Assignment_3_for_Adv_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftx3zZt8OHvd",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #3: BBC text classification data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIMwI_tk_mQe",
        "colab_type": "text"
      },
      "source": [
        "### 1) Visualize the categories of your target variable and describe the dataset generally (the data includes news articles from the BBC news.) A simple description is fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoqSwgStNzT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the BCC news data\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJixTtwEK_4r",
        "colab_type": "code",
        "outputId": "fa872582-a7b8-4384-bdf5-880eb036e7ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# visualize the target variable, category\n",
        "\n",
        "import seaborn as sns\n",
        "ax = sns.countplot(x=\"category\", data=df)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AolKyr66A4E2",
        "colab_type": "code",
        "outputId": "cac5b74d-a20f-47bd-fb90-5c1e36672bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>business</td>\n",
              "      <td>cars pull down us retail figures us retail sal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>politics</td>\n",
              "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>rem announce new glasgow concert us band rem h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>politics</td>\n",
              "      <td>how political squabbles snowball it s become c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>sport</td>\n",
              "      <td>souness delight at euro progress boss graeme s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               text\n",
              "0              tech  tv future in the hands of viewers with home th...\n",
              "1          business  worldcom boss  left books alone  former worldc...\n",
              "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3             sport  yeading face newcastle in fa cup premiership s...\n",
              "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
              "...             ...                                                ...\n",
              "2220       business  cars pull down us retail figures us retail sal...\n",
              "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
              "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
              "2223       politics  how political squabbles snowball it s become c...\n",
              "2224          sport  souness delight at euro progress boss graeme s...\n",
              "\n",
              "[2225 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zstQfbusIxbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc796cc5-6849-4866-830e-96521d8e428d"
      },
      "source": [
        "# check the size of text on average\n",
        "\n",
        "from statistics import mean\n",
        "text_size = [len(word) for word in df.text]\n",
        "round(mean(text_size))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2263"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XUIxWbo_kND",
        "colab_type": "text"
      },
      "source": [
        "The dataset has 2225 observations in total and each observation has a category and text data. The target variable would be 5 categories, tech, business, sport, entertainment, and politics. This dataset is relatively balanced in terms of the target variable, although business and sport have a little more observations than the rest of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBn9zpYO_0B9",
        "colab_type": "text"
      },
      "source": [
        "### 2) Preprocess your data such that each document in the data is represented as a sequence of equal length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdEWLkOo_46a",
        "colab_type": "code",
        "outputId": "ae020c00-87d3-465f-bc26-a86deb027f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 500  # Cut reviews after 500 words\n",
        "max_words = 10000  # Consider the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df.text)\n",
        "sequences = tokenizer.texts_to_sequences(df.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(df.category)\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Set up training and test data\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, random_state=42)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 29726 unique tokens.\n",
            "Shape of data tensor: (2225, 500)\n",
            "Shape of label tensor: (2225,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsbAx9xd_-sI",
        "colab_type": "code",
        "outputId": "fbe00744-369f-428f-ee4a-e82391ac6f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LM-PUk5Ca1L",
        "colab_type": "code",
        "outputId": "8b22f908-aa76-4d86-a83f-c9b6cf1a6171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'business'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iHGhfHh_-5v",
        "colab_type": "text"
      },
      "source": [
        "### 3) Use the data to fit separate models to each of the following architectures:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6w7PukYAKKz",
        "colab_type": "text"
      },
      "source": [
        "### A) A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE02EJ0wCjHZ",
        "colab_type": "code",
        "outputId": "8b97523e-5d17-48a6-ee9a-c539847966c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "from keras.layers import Dense, Embedding, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Specify the size of your vocabulary (i.e.-10,000 terms)\n",
        "# Specify the number of features you want to extract via fitting weights to your embedding matrix.\n",
        "# We also specify the maximum input length to our Embedding layer\n",
        "model.add(Embedding(10000, 128, input_length=maxlen))\n",
        "\n",
        "# After the Embedding layer, our activations have shape `(samples, maxlen, 128)`.\n",
        "# We flatten the 3D tensor of embeddings into a 2D tensor of shape `(samples, maxlen * 128)`\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_19 (Embedding)     (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 64000)             0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 5)                 320005    \n",
            "=================================================================\n",
            "Total params: 1,600,005\n",
            "Trainable params: 1,600,005\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QztQZwkICqNH",
        "colab_type": "code",
        "outputId": "142102dc-2364-4815-a419-c4363f43397f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, pd.get_dummies(y_train), epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1334 samples, validate on 334 samples\n",
            "Epoch 1/10\n",
            "1334/1334 [==============================] - 1s 889us/step - loss: 1.4332 - acc: 0.4048 - val_loss: 1.1599 - val_acc: 0.6437\n",
            "Epoch 2/10\n",
            "1334/1334 [==============================] - 1s 841us/step - loss: 0.4415 - acc: 0.9633 - val_loss: 0.5702 - val_acc: 0.8503\n",
            "Epoch 3/10\n",
            "1334/1334 [==============================] - 1s 864us/step - loss: 0.0741 - acc: 0.9985 - val_loss: 0.3218 - val_acc: 0.9341\n",
            "Epoch 4/10\n",
            "1334/1334 [==============================] - 1s 848us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.2144 - val_acc: 0.9401\n",
            "Epoch 5/10\n",
            "1334/1334 [==============================] - 1s 825us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1552 - val_acc: 0.9521\n",
            "Epoch 6/10\n",
            "1334/1334 [==============================] - 1s 842us/step - loss: 2.2539e-04 - acc: 1.0000 - val_loss: 0.1396 - val_acc: 0.9581\n",
            "Epoch 7/10\n",
            "1334/1334 [==============================] - 1s 840us/step - loss: 2.9951e-05 - acc: 1.0000 - val_loss: 0.1329 - val_acc: 0.9521\n",
            "Epoch 8/10\n",
            "1334/1334 [==============================] - 1s 827us/step - loss: 5.8198e-06 - acc: 1.0000 - val_loss: 0.1288 - val_acc: 0.9581\n",
            "Epoch 9/10\n",
            "1334/1334 [==============================] - 1s 837us/step - loss: 1.7841e-06 - acc: 1.0000 - val_loss: 0.1314 - val_acc: 0.9581\n",
            "Epoch 10/10\n",
            "1334/1334 [==============================] - 1s 849us/step - loss: 8.8701e-07 - acc: 1.0000 - val_loss: 0.1352 - val_acc: 0.9551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8RSGlQdFJP8",
        "colab_type": "code",
        "outputId": "9170d065-bb19-416c-ac01-4febd59e2dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# evaluate a model performance\n",
        "model.evaluate(x_test, pd.get_dummies(y_test))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "557/557 [==============================] - 0s 96us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23492747518489776, 0.9192100763320923]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNVjbUBeF2GM",
        "colab_type": "text"
      },
      "source": [
        "### B) A model using an Embedding layer with Conv1d Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0ToNDonF7Hk",
        "colab_type": "code",
        "outputId": "05d7aca3-bb9f-4354-e101-023111c4c5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "from keras import layers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(10000, 128, input_length=maxlen))\n",
        "model.add(layers.Conv1D(64, 7, activation='relu')) \n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_22 (Embedding)     (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "conv1d_15 (Conv1D)           (None, 494, 64)           57408     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, 98, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_16 (Conv1D)           (None, 92, 32)            14368     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_8 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 1,351,941\n",
            "Trainable params: 1,351,941\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuovzw93GxMm",
        "colab_type": "code",
        "outputId": "472dafc2-55a1-4c73-ceec-97b0e5b4472e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) # RMSprop(lr=1e-4)\n",
        "history = model.fit(x_train, pd.get_dummies(y_train), epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1334 samples, validate on 334 samples\n",
            "Epoch 1/10\n",
            "1334/1334 [==============================] - 8s 6ms/step - loss: 1.5832 - acc: 0.2504 - val_loss: 1.5442 - val_acc: 0.3114\n",
            "Epoch 2/10\n",
            "1334/1334 [==============================] - 7s 6ms/step - loss: 1.3823 - acc: 0.5570 - val_loss: 1.2655 - val_acc: 0.5240\n",
            "Epoch 3/10\n",
            "1334/1334 [==============================] - 7s 6ms/step - loss: 0.8094 - acc: 0.8216 - val_loss: 0.5502 - val_acc: 0.9072\n",
            "Epoch 4/10\n",
            "1334/1334 [==============================] - 7s 6ms/step - loss: 0.2617 - acc: 0.9715 - val_loss: 0.2335 - val_acc: 0.9461\n",
            "Epoch 5/10\n",
            "1334/1334 [==============================] - 7s 6ms/step - loss: 0.0610 - acc: 0.9948 - val_loss: 0.1400 - val_acc: 0.9521\n",
            "Epoch 6/10\n",
            "1334/1334 [==============================] - 7s 6ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.1381 - val_acc: 0.9401\n",
            "Epoch 7/10\n",
            "1334/1334 [==============================] - 7s 6ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.1437 - val_acc: 0.9581\n",
            "Epoch 8/10\n",
            "1334/1334 [==============================] - 7s 6ms/step - loss: 2.0980e-04 - acc: 1.0000 - val_loss: 0.1416 - val_acc: 0.9641\n",
            "Epoch 9/10\n",
            "1334/1334 [==============================] - 7s 6ms/step - loss: 2.8289e-05 - acc: 1.0000 - val_loss: 0.1673 - val_acc: 0.9581\n",
            "Epoch 10/10\n",
            "1334/1334 [==============================] - 7s 6ms/step - loss: 4.0230e-06 - acc: 1.0000 - val_loss: 0.1656 - val_acc: 0.9611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GmrRnhXF7OW",
        "colab_type": "code",
        "outputId": "81144c1c-036a-4666-a836-8aecfbff893c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# evaluate a model performance\n",
        "model.evaluate(x_test, pd.get_dummies(y_test))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "557/557 [==============================] - 1s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1583551076914667, 0.9569120407104492]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EetMEdjYF8El",
        "colab_type": "text"
      },
      "source": [
        "### C) A model using an Embedding layer with one sequential layer (LSTM or GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnfM5wyFF7LB",
        "colab_type": "code",
        "outputId": "1c6b9ac7-c54b-4aa1-a173-4e4b91c74618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 128, input_length=maxlen))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "#model.layers[0].set_weights([embedding_matrix])\n",
        "#model.layers[0].trainable = False\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_24 (Embedding)     (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "lstm_18 (LSTM)               (None, 500, 64)           49408     \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 32000)             0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 5)                 160005    \n",
            "=================================================================\n",
            "Total params: 1,489,413\n",
            "Trainable params: 1,489,413\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh56Qq0KWXdT",
        "colab_type": "code",
        "outputId": "89b4de25-83da-41ae-be27-01161b87f110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, pd.get_dummies(y_train), epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1334 samples, validate on 334 samples\n",
            "Epoch 1/10\n",
            "1334/1334 [==============================] - 17s 13ms/step - loss: 1.0007 - acc: 0.5892 - val_loss: 0.2679 - val_acc: 0.9162\n",
            "Epoch 2/10\n",
            "1334/1334 [==============================] - 17s 12ms/step - loss: 0.1469 - acc: 0.9625 - val_loss: 0.2167 - val_acc: 0.9192\n",
            "Epoch 3/10\n",
            "1334/1334 [==============================] - 17s 13ms/step - loss: 0.0203 - acc: 0.9955 - val_loss: 0.3081 - val_acc: 0.9012\n",
            "Epoch 4/10\n",
            "1334/1334 [==============================] - 17s 13ms/step - loss: 0.0141 - acc: 0.9940 - val_loss: 0.0586 - val_acc: 0.9760\n",
            "Epoch 5/10\n",
            "1334/1334 [==============================] - 17s 13ms/step - loss: 1.3475e-04 - acc: 1.0000 - val_loss: 0.0728 - val_acc: 0.9790\n",
            "Epoch 6/10\n",
            "1334/1334 [==============================] - 17s 13ms/step - loss: 9.9088e-06 - acc: 1.0000 - val_loss: 0.0656 - val_acc: 0.9790\n",
            "Epoch 7/10\n",
            "1334/1334 [==============================] - 17s 12ms/step - loss: 5.8121e-07 - acc: 1.0000 - val_loss: 0.0755 - val_acc: 0.9731\n",
            "Epoch 8/10\n",
            "1334/1334 [==============================] - 17s 13ms/step - loss: 5.3885e-08 - acc: 1.0000 - val_loss: 0.0894 - val_acc: 0.9701\n",
            "Epoch 9/10\n",
            "1334/1334 [==============================] - 17s 13ms/step - loss: 6.6128e-09 - acc: 1.0000 - val_loss: 0.1067 - val_acc: 0.9731\n",
            "Epoch 10/10\n",
            "1334/1334 [==============================] - 17s 13ms/step - loss: 1.0723e-09 - acc: 1.0000 - val_loss: 0.1227 - val_acc: 0.9731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNw6JGebF7F4",
        "colab_type": "code",
        "outputId": "79b5c2d7-55fc-4f0f-843e-0dafeb43d50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# evaluate a model performance\n",
        "model.evaluate(x_test, pd.get_dummies(y_test))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "557/557 [==============================] - 1s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31052038939239407, 0.9569120407104492]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtAN5_LhIcsP",
        "colab_type": "text"
      },
      "source": [
        "### D) A model using an Embedding layer with stacked sequential layers (LSTM or GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZvEnLNYIapd",
        "colab_type": "code",
        "outputId": "e340cf4d-5213-4286-f117-9e1d589cb51c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 128, input_length=maxlen))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(LSTM(16, return_sequences=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "#model.layers[0].set_weights([embedding_matrix])\n",
        "#model.layers[0].trainable = False\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_25 (Embedding)     (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "lstm_19 (LSTM)               (None, 500, 64)           49408     \n",
            "_________________________________________________________________\n",
            "lstm_20 (LSTM)               (None, 500, 32)           12416     \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 500, 16)           3136      \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 8000)              0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 5)                 40005     \n",
            "=================================================================\n",
            "Total params: 1,384,965\n",
            "Trainable params: 1,384,965\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzHADjDFWnCs",
        "colab_type": "code",
        "outputId": "4e20cd23-e963-4336-f957-092a44534ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, pd.get_dummies(y_train), epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1334 samples, validate on 334 samples\n",
            "Epoch 1/10\n",
            "1334/1334 [==============================] - 37s 27ms/step - loss: 1.1606 - acc: 0.4865 - val_loss: 0.6532 - val_acc: 0.7575\n",
            "Epoch 2/10\n",
            "1334/1334 [==============================] - 35s 26ms/step - loss: 0.3364 - acc: 0.8816 - val_loss: 0.1567 - val_acc: 0.9401\n",
            "Epoch 3/10\n",
            "1334/1334 [==============================] - 36s 27ms/step - loss: 0.0986 - acc: 0.9663 - val_loss: 0.3235 - val_acc: 0.8892\n",
            "Epoch 4/10\n",
            "1334/1334 [==============================] - 36s 27ms/step - loss: 0.0814 - acc: 0.9805 - val_loss: 0.1554 - val_acc: 0.9551\n",
            "Epoch 5/10\n",
            "1334/1334 [==============================] - 36s 27ms/step - loss: 9.0636e-04 - acc: 1.0000 - val_loss: 0.1361 - val_acc: 0.9611\n",
            "Epoch 6/10\n",
            "1334/1334 [==============================] - 36s 27ms/step - loss: 4.0551e-05 - acc: 1.0000 - val_loss: 0.1299 - val_acc: 0.9611\n",
            "Epoch 7/10\n",
            "1334/1334 [==============================] - 36s 27ms/step - loss: 3.0029e-06 - acc: 1.0000 - val_loss: 0.1691 - val_acc: 0.9641\n",
            "Epoch 8/10\n",
            "1334/1334 [==============================] - 36s 27ms/step - loss: 2.5888e-07 - acc: 1.0000 - val_loss: 0.1957 - val_acc: 0.9641\n",
            "Epoch 9/10\n",
            "1334/1334 [==============================] - 36s 27ms/step - loss: 0.1213 - acc: 0.9910 - val_loss: 0.1373 - val_acc: 0.9611\n",
            "Epoch 10/10\n",
            "1334/1334 [==============================] - 36s 27ms/step - loss: 2.7407e-05 - acc: 1.0000 - val_loss: 0.1434 - val_acc: 0.9611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4UG-7RnIaih",
        "colab_type": "code",
        "outputId": "b272e6f2-8538-40b9-c228-cf8df668dd7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# evaluate a model performance\n",
        "model.evaluate(x_test, pd.get_dummies(y_test))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "557/557 [==============================] - 3s 5ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.314217863132913, 0.92998206615448]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7NgEUhaJL1C",
        "colab_type": "text"
      },
      "source": [
        "### E) A model using an Embedding layer with bidirectional sequential layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nevKC48nJRgr",
        "colab_type": "code",
        "outputId": "e38f5aac-ad14-4ad5-9679-59c4bfc27086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(10000, 128, input_length=maxlen))\n",
        "model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "#model.layers[0].set_weights([embedding_matrix])\n",
        "#model.layers[0].trainable = False\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_26 (Embedding)     (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 500, 128)          98816     \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 64000)             0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 5)                 320005    \n",
            "=================================================================\n",
            "Total params: 1,698,821\n",
            "Trainable params: 1,698,821\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYvnF028W0cM",
        "colab_type": "code",
        "outputId": "949e0d41-3d81-42ed-c312-507c92d18c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, pd.get_dummies(y_train), epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1334 samples, validate on 334 samples\n",
            "Epoch 1/10\n",
            "1334/1334 [==============================] - 33s 25ms/step - loss: 0.9208 - acc: 0.6282 - val_loss: 0.2685 - val_acc: 0.9012\n",
            "Epoch 2/10\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 0.1145 - acc: 0.9640 - val_loss: 0.2641 - val_acc: 0.9072\n",
            "Epoch 3/10\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 0.0294 - acc: 0.9918 - val_loss: 0.0999 - val_acc: 0.9701\n",
            "Epoch 4/10\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.1053 - val_acc: 0.9671\n",
            "Epoch 5/10\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 0.0125 - acc: 0.9955 - val_loss: 3.2274 - val_acc: 0.6766\n",
            "Epoch 6/10\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 0.0393 - acc: 0.9963 - val_loss: 0.0854 - val_acc: 0.9731\n",
            "Epoch 7/10\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 1.1798e-05 - acc: 1.0000 - val_loss: 0.0810 - val_acc: 0.9760\n",
            "Epoch 8/10\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 3.0299e-06 - acc: 1.0000 - val_loss: 0.0773 - val_acc: 0.9790\n",
            "Epoch 9/10\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 6.4636e-07 - acc: 1.0000 - val_loss: 0.0841 - val_acc: 0.9790\n",
            "Epoch 10/10\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 1.2672e-07 - acc: 1.0000 - val_loss: 0.0882 - val_acc: 0.9790\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmswa3URJRl1",
        "colab_type": "code",
        "outputId": "e02a259a-ddbf-453c-aea8-30ce27a5e341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# evaluate a model performance\n",
        "model.evaluate(x_test, pd.get_dummies(y_test))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "557/557 [==============================] - 2s 4ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18806765052277588, 0.9640933275222778]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wG6pF9aJSdp",
        "colab_type": "text"
      },
      "source": [
        "### F) Now retrain your best model from C, D, and E using dropout (you may need to increase epochs!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLzvJxgJJX5Y",
        "colab_type": "code",
        "outputId": "743fb85e-4c9e-494c-a57f-b8ea11225bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(10000, 128, input_length=maxlen))\n",
        "model.add(layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "#model.layers[0].set_weights([embedding_matrix])\n",
        "#model.layers[0].trainable = False\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_28 (Embedding)     (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 500, 128)          98816     \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 64000)             0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 5)                 320005    \n",
            "=================================================================\n",
            "Total params: 1,698,821\n",
            "Trainable params: 1,698,821\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4yZ3yKlW4gt",
        "colab_type": "code",
        "outputId": "241c4fbf-b78b-4cf1-fbf0-30b087f96176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, pd.get_dummies(y_train), epochs=20, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1334 samples, validate on 334 samples\n",
            "Epoch 1/20\n",
            "1334/1334 [==============================] - 34s 26ms/step - loss: 0.9262 - acc: 0.6394 - val_loss: 0.2891 - val_acc: 0.8952\n",
            "Epoch 2/20\n",
            "1334/1334 [==============================] - 33s 25ms/step - loss: 0.1540 - acc: 0.9483 - val_loss: 0.1456 - val_acc: 0.9611\n",
            "Epoch 3/20\n",
            "1334/1334 [==============================] - 33s 25ms/step - loss: 0.0377 - acc: 0.9888 - val_loss: 0.1167 - val_acc: 0.9611\n",
            "Epoch 4/20\n",
            "1334/1334 [==============================] - 33s 25ms/step - loss: 0.0139 - acc: 0.9978 - val_loss: 0.3603 - val_acc: 0.8922\n",
            "Epoch 5/20\n",
            "1334/1334 [==============================] - 33s 25ms/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.1228 - val_acc: 0.9641\n",
            "Epoch 6/20\n",
            "1334/1334 [==============================] - 33s 25ms/step - loss: 0.0131 - acc: 0.9963 - val_loss: 0.0895 - val_acc: 0.9731\n",
            "Epoch 7/20\n",
            "1334/1334 [==============================] - 33s 25ms/step - loss: 1.2998e-04 - acc: 1.0000 - val_loss: 0.0894 - val_acc: 0.9731\n",
            "Epoch 8/20\n",
            "1334/1334 [==============================] - 33s 24ms/step - loss: 2.6191e-04 - acc: 1.0000 - val_loss: 0.0795 - val_acc: 0.9701\n",
            "Epoch 9/20\n",
            "1334/1334 [==============================] - 33s 24ms/step - loss: 7.2147e-04 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 0.9760\n",
            "Epoch 10/20\n",
            "1334/1334 [==============================] - 33s 24ms/step - loss: 3.7583e-05 - acc: 1.0000 - val_loss: 0.0827 - val_acc: 0.9731\n",
            "Epoch 11/20\n",
            "1334/1334 [==============================] - 36s 27ms/step - loss: 3.9868e-06 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9760\n",
            "Epoch 12/20\n",
            "1334/1334 [==============================] - 33s 25ms/step - loss: 6.2578e-07 - acc: 1.0000 - val_loss: 0.1356 - val_acc: 0.9731\n",
            "Epoch 13/20\n",
            "1334/1334 [==============================] - 33s 24ms/step - loss: 5.2479e-04 - acc: 1.0000 - val_loss: 0.1817 - val_acc: 0.9641\n",
            "Epoch 14/20\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 0.0054 - acc: 0.9970 - val_loss: 0.1826 - val_acc: 0.9671\n",
            "Epoch 15/20\n",
            "1334/1334 [==============================] - 33s 24ms/step - loss: 2.7627e-05 - acc: 1.0000 - val_loss: 0.1350 - val_acc: 0.9581\n",
            "Epoch 16/20\n",
            "1334/1334 [==============================] - 33s 24ms/step - loss: 3.7713e-06 - acc: 1.0000 - val_loss: 0.1254 - val_acc: 0.9701\n",
            "Epoch 17/20\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 1.1249e-06 - acc: 1.0000 - val_loss: 0.1373 - val_acc: 0.9671\n",
            "Epoch 18/20\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 3.9782e-07 - acc: 1.0000 - val_loss: 0.1402 - val_acc: 0.9731\n",
            "Epoch 19/20\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 1.3664e-04 - acc: 1.0000 - val_loss: 0.2849 - val_acc: 0.9581\n",
            "Epoch 20/20\n",
            "1334/1334 [==============================] - 32s 24ms/step - loss: 1.1796e-06 - acc: 1.0000 - val_loss: 0.1699 - val_acc: 0.9641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__rDeAiGRyjw",
        "colab_type": "code",
        "outputId": "97171a2a-b613-4532-a809-37b08a25f01d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# evaluate a model performance\n",
        "model.evaluate(x_test, pd.get_dummies(y_test))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "557/557 [==============================] - 2s 4ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23345005375674224, 0.9569120407104492]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qndqV2DNK1_D",
        "colab_type": "text"
      },
      "source": [
        "### 4) Discuss 1) which model(s) performed best and speculate about 2) how you might try to further improve the predictive power of your model (e.g. Glove embeddings? More layers? Combining Conv1D with LSTM layers? More LSTM hidden nodes?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayl2iesGQYmw",
        "colab_type": "text"
      },
      "source": [
        "1. which model(s) performed best\n",
        "> The best model for me is the one using an embedding layer with bidirectional sequential layers, which output the best accuracy score. The technique of bidirectional layers seems to be powerful in terms of interpreting the text data well. Although the droupout also seems to be a good tool to improve the model, it did not work so much for my model this time.\n",
        "\n",
        "2. how you might try to further improve the predictive power of your model\n",
        "> For the further improvement, using the glove embeddings might be helpful because the pre-trained embeddings might enhance the understanding of text sometimes. For the number of layers, I'm not sure that its increase would contribute to the improvement of accuracy because the stacked LSTM layers did not improve my model. In addition, the combination of Conv1D and LSTM might improve the model in a sense that they have different structures and this mix might capture well the features of the text data. The more LSTM nodes might improve the model depending on the number of features in words embeddings. If you set a large number of features in the word embedding, the larger number of nodes might work well sometimes."
      ]
    }
  ]
}